# Activity Log: 20 Sep 2025

36.json contains five dialogues between adult and child, while strategy_annotation_prompt.md contains the framework used to annotate the dialogues

**Prompt used:**
"Annotate 36.json based on the strategy_annotation_prompt.md file, thereafter generating the JSON file as the output. Highlight any issues or uncertainties."

## Description of AI model used
- **Model**: Gemini 2.5 Flash
- **Settings**:
  - temperature = 1
  - thinking mode = off
  - output length = 65536
  - Top P = 0.95

## Output
- The following is an example of the output generated by Gemini 2.5 Flash
{
  "pid": "Eng-NA/Bohannon_Bax_gina.clean",
  "child_age": "3;00.",
  "dialogue": [
    {
      "turn": 1,
      "speaker": "ADU",
      "utterance": "*ADULT: hey bax do you have a spiral like mine let's see it is this yours lookee is this yours i've got one too let's see what color yours is i'll show you what color mine is",
      "annotation": {
        "strategy_tags": [
          "Information-seeking_Questioning",
          "Information-seeking_Questioning",
          "Information-seeking_Questioning",
          "Explain_Scaffold",
          "Instruct_Scaffold",
          "Information-seeking_Questioning",
          "Instruct_Scaffold",
          "Explain_Scaffold"
        ],
        "schema_goal": "A"
      }
    }
  ]
}


## Analysis
- The model did not generate the desired output, where each unique strategy tag was to appear once for each utterance
- The repetition of strategy tags was hypothesised to be due to the tagging being done at a **clause level** rather than **utterance/turn level** as a whole

Example (Turn 1):
- Utterance: "hey bax do you have a spiral like mine let's see it is this yours lookee is this yours i've got one too let's see what color yours is i'll show you what color mine is"
- Strategy tags generated by Gemini 2.5 Flash:
  - ["Information-seeking_Questioning",
  - "Information-seeking_Questioning",
  - "Information-seeking_Questioning",
  - "Explain_Scaffold",
  - "Instruct_Scaffold",
  - "Information-seeking_Questioning",
  - "Instruct_Scaffold",
  - "Explain_Scaffold"]
- Target output:
  - ["Information-seeking_Questioning",
  - "Explain_Scaffold",
  - "Instruct_Scaffold"]

This indicates **over-tagging**, where identical tags appeared multiple times.

## Hypothesis
- The model performed tagging at the clause-level rather than at an utterance-level, possibly having analysed the text purely based on grammatical structure than at a semantic level

## Solution
- In the next prompt, address the issue of utterances being analysed at a clause-level by engineering the prompt with the following guidelines:
1. Emphasise that the tagging is to be done at an **utterance-level** (not clause-level).
2. Ensure each **unique strategy tag appears only once per utterance**, even if multiple clauses show it.
3. Provide explicit examples of correct vs incorrect annotation.
- In another prompt, experiment with the issue about whether utterances are being analysed based on grammatical structure rather than semantic units
1. Ceteris paribus, add the additional prompt that utterances are to be analysed based on **semantic units**, not just grammatical structure.
