# 3 Nov 2025 - Test 9

### **Settings**
- **Environment:** Google AI Studio  
- **Model:** Gemini 2.5 Flash Lite  
- **Files used:**  
  - `strategy_annotation_prompt_v0.3.md`  
  - `36 (1 Bohannon_Bax_gina)_first100.json`  
  - `2025-10-28-annotation-test-7a_36 json_temp 0.2.json`  
  - `2025-10-28-annotation-test-7b_36 json_temp 1.json`  
  - `2025-10-29-annotation-test-8_36 json_temp 0.5.json`  

### Overview
I manually validated and compared the annotation outputs provided by the model based on the three **temperatures: 0.2, 0.5, and 1**.  
I performed this comparison to understand the trends in **how temperature affects the model's tagging and its accuracy**.  
Two prompts were used in this test: the initial prompt and follow-up prompt (see below)

- Temperature 0.2 tended to produce more **precise and exhaustive strategy tags** and more accurate child alignment levels.  
- Temperature 0.5 struck a **balance between precision and variability**, sometimes omitting secondary tags.  
- Temperature 1.0 showed **more creative but less precise tagging**, occasionally misapplying strategy types or schema goals.  

#### Sequence for Clarity  
1. Initial prompt  
2. Model Response 1  
3. Follow-up prompt  
4. Model response 2  


### Summary of Findings
- Temperature 0.2 output is **recommended for accurate strategy annotation and child alignment**, with 0.5 as a secondary choice for minor schema goal advantages.  
- Temperature 1.0 adds creative variability but at the cost of accuracy.  

---

### Initial Prompt I Used (in Google AI Studio)  
> “I analysed `36 (1 Bohannon_Bax_gina)_first100.json` using `strategy_annotation_prompt_v0.3.md`. I ran this test thrice via Colab using Gemini 2.5 Flash Lite, once with temperature 0.2 (output: `2025-10-28-annotation-test-7a_36 json_temp 0.2.json`), once with temperature 0.5 (`2025-10-29-annotation-test-8_36 json_temp 0.5.json`), and once with temperature 1 (`2025-10-28-annotation-test-7b_36 json_temp 1.json`). Analyse the differences between the three outputs. What are the trends for each output, how could temperature have affected them, and which has the most accurate tagging based on human understanding of conversations?”  

---

### Model Response #1
**Turn-by-Turn Differences:**  

- **Turn 3 (ADU):** "do you know how to do patty cake"  
  - 0.2 & 0.5: Memory-prompting_Questioning (T)  
  - 1.0: Information-seeking_Questioning (A)  
  **Analysis:** 0.2 more accurately prompts recall; 1.0 misinterprets as seeking new info.  

- **Turn 11 (ADU):** "mine's yellow … what color could it be"  
  - 0.2: Explain_Scaffold, Feedback_Scaffold, Thought-provoking_Questioning (T)  
  - 0.5: Explain_Scaffold, Feedback_Scaffold, Thought-provoking_Questioning (R)  
  - 1.0: Explain_Scaffold, Instruct_Scaffold, Thought-provoking_Questioning (R)  
  **Analysis:** 0.5 & 1.0 more accurate for schema goal (Restructuring) due to child error; strategy tagging slightly varies.  

- **Turn 19 (ADU):** "yellow um i bet this would go in there"  
  - 0.2 & 0.5: Explain_Scaffold (A)  
  - 1.0: Explain_Scaffold, Instruct_Scaffold (T)  
  **Analysis:** 1.0 adds Instruct and changes schema to T; plausible but less precise than 0.2.  

- **Turn 21 (ADU):** "do you wanna get some more toys out"  
  - 0.2: Confirmation_Questioning (T)  
  - 0.5 & 1.0: Information-seeking_Questioning (T for 0.5, A for 1.0)  
  **Analysis:** 0.2 correct; others misapply definitions.  

- **Turn 41 (ADU):** "let's see why don't you come back this way"  
  - 0.2: Instruct_Scaffold, Explain_Scaffold, Model_Scaffold (T)  
  - 0.5: Instruct_Scaffold, Explain_Scaffold (T)  
  - 1.0: Instruct_Scaffold, Explain_Scaffold, Feedback_Scaffold (T)  

- **Turn 47 (ADU):** "can you do patty cake"  
  - 0.2: Confirmation_Questioning, Memory-prompting_Questioning (T)  
  - 0.5 & 1.0: Confirmation_Questioning (T)  

- **Turn 79 (ADU):** "what bax huh oh no he ran over the hat"  
  - 0.2 & 0.5: Information-seeking_Questioning, Explain_Scaffold (T/A)  
  - 1.0: Explain_Scaffold, Social-emotional-support (A)  

- **Turn 92 (CHI):** "my toy is over there"  
  - 0.2: disalign  
  - 0.5 & 1.0: partial  

- **Turn 98 (CHI):** "my ball!"  
  - 0.2: disalign  
  - 0.5: partial  
  - 1.0: unknown  

**Trends:**  
- 0.2 → conservative, precise, complete, accurate child alignment  
- 0.5 → balanced, minor omissions  
- 1.0 → more variable, less precise, occasionally misapplied strategy tags  

**Accuracy Conclusion:**  
- Temperature 0.2 is the most accurate for strategy and alignment, despite minor exceptions (e.g., Turn 98).  

---

### Follow-up Prompt I Used
> “How would you rate the accuracy of each output? Are you sure temp 0.2 is the most accurate and not 0.5? Why?”  

---

### Model Response #2
**Accuracy Ratings of Annotations by Temperature, Predicted by Gemini:**  

| Temperature | Accuracy | Trend |
|------------|---------|-------|
| 0.2 | ~90–95% | Highly precise, exhaustive, nuanced strategy differentiation, accurate child alignment |
| 0.5 | ~80–85% | Good, minor inconsistencies, secondary tags sometimes missing |
| 1.0 | ~70–75% | Moderate-high, more variability, less precise, occasional misapplied tags |

**Justification for 0.2:**  
- **Turn 21:** 0.2 correctly tags Confirmation_Questioning vs 0.5 mislabeling  
- **Turn 41 & 47:** 0.2 includes Model_Scaffold & Memory-prompting for completeness  
- **Turn 79 & 92:** Better schema & alignment accuracy  
- **Overall:** 0.2 provides the most precise, complete, and logically sound annotations, consistently aligning with human judgment.  

**Conclusion:**  
- In summary, while 0.5 occasionally makes a better schema goal choice, 0.2 consistently provides more precise strategy tags, more complete tag sets, and more accurate alignment levels for child utterances across a wider range of turns.
- The overall quality and adherence to the detailed guidelines are higher in the 0.2 output.  
